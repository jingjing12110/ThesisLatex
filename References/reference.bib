@String(CVPR= {IEEE Conference on Computer Vision and Pattern Recognition})
@String(ICCV= {IEEE International Conference on Computer Vision})
@String(ECCV= {European Conference on Computer Vision})
@String(ICPR = {International Conference on Pattern Recognition})
@String(BMVC= {British Machine Vision Conference})
@String(WACV= {IEEE Winter Conference on Applications of Computer Vision})
@String(ACCV  = {Asian Conference on Computer Vision})

@String(AAAI = {AAAI Conference on Artificial Intelligence})
@String(IJCAI = {International Joint Conferences on Artificial Intelligence Organization})

@String(MM= {ACM International Conference on Multimedia})
@String(ICME = {IEEE International Conference on Multimedia and Expo})

@String(ACL= {Annual Meeting of the Association for Computational Linguistics})
@String(EMNLP= {Conference on Empirical Methods in Natural Language Processing})
@String(ICASSP=	{IEEE International Conference on Acoustics, Speech and Signal Processing})
@String(ICIP = {IEEE International Conference on Image Processing})
@String(KDD  = {ACM SIGKDD Conference on Knowledge Discovery and Data Mining})
@String(WWW  = {International World Wide Web Conferences})
@String(SIGIR = {International ACM SIGIR Conference on Research and Development in Information Retrieval})

@String(ICLR = {International Conference on Learning Representations})
@String(ICML  =	{International Conference on Machine Learning})
@String(NIPS= {Advances in Neural Information Processing Systems})

@String(TPAMI = {IEEE Transactions on Pattern Analysis and Machine Intelligence})
@String(IJCV = {International Journal of Computer Vision})
@String(TIP  = {IEEE Transactions on Image Processing})
@String(TMM  = {IEEE Transactions on Multimedia})
@String(TOMM  = {ACM Transactions on Multimedia Computing, Communications, and Applications})
@String(TOG= {ACM Transactions on Graphics})
@String(TVCG  = {IEEE Transactions on Visualization and Computer Graphics})

@String(PR   = {Pattern Recognition})
@String(CSVT = {IEEE Transactions on Circuits and Systems for Video Technology})
@String(SPL	= {IEEE Signal Processing Letters})
@String(SPM = {IEEE Signal Processing Magazine})
@String(TNN  = {IEEE Transactions on Neural Networks and Learning Systems})


% ***********************************************************************
% Introduction
@article{li2022vision,
	title   = {Vision-language intelligence: Tasks, representation learning, and large models},
	author  = {Li, Feng and Zhang, Hao and Zhang, Yi-Fan and Liu, Shilong and Guo, Jian and Ni, Lionel M and Zhang, PengChuan and Zhang, Lei},
	journal = {arXiv preprint arXiv:2203.01922},
	year    = {2022}
}
@article{hassan2018achieving,
	title   = {Achieving human parity on automatic chinese to english news translation},
	author  = {Hassan, Hany and Aue, Anthony and Chen, Chang and Chowdhary, Vishal and Clark, Jonathan and Federmann, Christian and Huang, Xuedong and Junczys-Dowmunt, Marcin and Lewis, William and Li, Mu and others},
	journal = {arXiv preprint arXiv:1803.05567},
	year    = {2018}
}
@inproceedings{malinowski2014multi,
	title     = {A multi-world approach to question answering about real-world scenes based on uncertain input},
	author    = {Malinowski, Mateusz and Fritz, Mario},
	booktitle   = NIPS,
	publisher = {The MIT Press},
	address   = {Montreal, QC, Canada},
	pages     = {1682--1690},
	year      = {2014}
}
@article{malinowski2014towards,
	title   = {Towards a visual turing challenge},
	author  = {Malinowski, Mateusz and Fritz, Mario},
	journal = {arXiv preprint arXiv:1410.8027},
	year    = {2014}
}
@inproceedings{kim2016hadamard,
	title   = {Hadamard product for low-rank bilinear pooling},
	author  = {Kim, Jin-Hwa and On, Kyoung-Woon and Lim, Woosang and Kim, Jeonghee and Ha, Jung-Woo and Zhang, Byoung-Tak},
	booktitle = ICLR,
	publisher = {Openreview},
	address   = {Toulon, France},
	year      = {2017}
}
@inproceedings{fukui2016multimodal,
	title     = {Multimodal compact bilinear pooling for visual question answering and visual grounding},
	author    = {Fukui, Akira and Park, Dong Huk and Yang, Daylen and Rohrbach, Anna and Darrell, Trevor and Rohrbach, Marcus},
	booktitle = EMNLP,
	publisher = {ACL},
	address   = {Austin, TX, USA},
	pages     = {457--468},
	year      = {2016}
}
@inproceedings{yu2017multi,
	title     = {Multi-modal factorized bilinear pooling with co-attention learning for visual question answering},
	author    = {Yu, Zhou and Yu, Jun and Fan, Jianping and Tao, Dacheng},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Honolulu, HI, USA},
	pages     = {1821--1830},
	year      = {2017}
}
@inproceedings{ben2017mutan,
	title     = {Mutan: Multimodal tucker fusion for visual question answering},
	author    = {Ben-Younes, Hedi and Cadene, R{\'e}mi and Cord, Matthieu and Thome, Nicolas},
	booktitle = ICCV,
	publisher = {IEEE},
	address   = {Venice, Italy},
	pages     = {2612--2620},
	year      = {2017}
}
@inproceedings{xu2016ask,
	title     = {Ask, attend and answer: Exploring question-guided spatial attention for visual question answering},
	author    = {Xu, Huijuan and Saenko, Kate},
	booktitle = ECCV,
	publisher = {Springer},
	address   = {Amsterdam, Netherlands},
	pages     = {451--466},
	year      = {2016}
}
@inproceedings{patro2018differential,
	title     = {Differential attention for visual question answering},
	author    = {Patro, Badri and Namboodiri, Vinay P},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Salt Lake City, UT, USA},
	pages     = {7680--7688},
	year      = {2018}
}
@inproceedings{lu2016hierarchical,
	title     = {Hierarchical question-image co-attention for visual question answering},
	author    = {Lu, Jiasen and Yang, Jianwei and Batra, Dhruv and Parikh, Devi},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Barcelona, Spain},
	pages     = {289--297},
	year      = {2016}
}
@inproceedings{nam2017dual,
	title     = {Dual attention networks for multimodal reasoning and matching},
	author    = {Nam, Hyeonseob and Ha, Jung-Woo and Kim, Jeonghee},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Honolulu, HI, USA},
	pages     = {299--307},
	year      = {2017}
}
@inproceedings{nguyen2018improved,
	title     = {Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering},
	author    = {Nguyen, Duy-Kien and Okatani, Takayuki},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Salt Lake City, UT, USA},
	pages     = {6087--6096},
	year      = {2018}
}
@inproceedings{gao2019dynamic,
	title     = {Dynamic fusion with intra-and inter-modality attention flow for visual question answering},
	author    = {Gao, Peng and Jiang, Zhengkai and You, Haoxuan and Lu, Pan and Hoi, Steven CH and Wang, Xiaogang and Li, Hongsheng},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Long Beach, CA, USA},
	pages     = {6639--6648},
	year      = {2019}
}
@inproceedings{andreas2016neural,
	title     = {Neural module networks},
	author    = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Las Vegas, NV, USA},
	pages     = {39--48},
	year      = {2016}
}
@inproceedings{hu2017learning,
	title     = {Learning to reason: End-to-end module networks for visual question answering},
	author    = {Hu, Ronghang and Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Saenko, Kate},
	booktitle = ICCV,
	publisher = {IEEE},
	address   = {Venice, Italy},
	pages     = {804--813},
	year      = {2017}
}
@inproceedings{akula2021robust,
	title     = {Robust visual reasoning via language guided neural module networks},
	author    = {Akula, Arjun and Jampani, Varun and Changpinyo, Soravit and Zhu, Song-Chun},
	booktitle   = NIPS,
	publisher = {The MIT Press},
	address   = {Virtual},
	pages     = {11041--11053},
	year      = {2021}
}
@inproceedings{shang2017video,
	title     = {Video visual relation detection},
	author    = {Shang, Xindi and Ren, Tongwei and Guo, Jingfan and Zhang, Hanwang and Chua, Tat-Seng},
	booktitle = MM,
	publisher = {ACM},
	address   = {New York, NY, USA},
	pages     = {1300--1308},
	year      = {2017}
}
@inproceedings{lu2016visual,
	title     = {Visual relationship detection with language priors},
	author    = {Lu, Cewu and Krishna, Ranjay and Bernstein, Michael and Fei-Fei, Li},
	booktitle = ECCV,
	publisher = {Springer},
	address   = {Amsterdam, Netherlands},
	pages     = {852--869},
	year      = {2016}
}
@inproceedings{wu2018object,
	title     = {Object-difference attention: A simple relational attention for visual question answering},
	author    = {Wu, Chenfei and Liu, Jinlai and Wang, Xiaojie and Dong, Xuan},
	booktitle = MM,
	publisher = {ACM},
	address   = {Seoul, Korea},
	pages     = {519--527},
	year      = {2018}
}
@inproceedings{cho2021unifying,
	title     = {Unifying vision-and-language tasks via text generation},
	author    = {Cho, Jaemin and Lei, Jie and Tan, Hao and Bansal, Mohit},
	booktitle = ICML,
	publisher = {ACM},
	address   = {Virtual},
	pages     = {1931--1942},
	year      = {2021}
}
@article{liu2021unified,
	title   = {Unified multimodal pre-training and prompt-based tuning for vision-language understanding and generation},
	author  = {Liu, Tianyi and Wu, Zuxuan and Xiong, Wenhan and Chen, Jingjing and Jiang, Yu-Gang},
	journal = {arXiv preprint arXiv:2112.05587},
	year    = {2021}
}
@article{yang2021crossing,
	title   = {Crossing the format boundary of text and boxes: towards unified vision-language modeling},
	author  = {Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Ahmed, Faisal and Liu, Zicheng and Lu, Yumao and Wang, Lijuan},
	journal = {arXiv preprint arXiv:2111.12085},
	year    = {2021}
}
@inproceedings{jin2022good,
	title     = {A good prompt is worth millions of parameters: Low-resource prompt-based learning for vision-language models},
	author    = {Jin, Woojeong and Cheng, Yu and Shen, Yelong and Chen, Weizhu and Ren, Xiang},
	booktitle = ACL,
	publisher = {ACL},
	address   = {Dublin, Ireland},
	pages     = {2763--2775},
	year      = {2022}
}



% LiVLR: VideoQA
@inproceedings{kim2018multimodal,
	title     = {Multimodal dual attention memory for video story question answering},
	author    = {Kim, Kyung-Min and Choi, Seong-Ho and Kim, Jin-Hwa and Zhang, Byoung-Tak},
	booktitle = ECCV,
	publisher = {Springer},
	address   = {Munich, Germany},
	pages     = {673--688},
	year      = {2018}
}
@inproceedings{kim2020dense,
	title     = {Dense-caption matching and frame-selection gating for temporal localization in videoqa},
	author    = {Kim, Hyounghun and Tang, Zineng and Bansal, Mohit},
	booktitle = ACL,
	publisher = {ACL},
	address   = {Virtual},
	pages     = {4812--4822},
	year      = {2020}
}
@inproceedings{xu2017video,
	title     = {Video question answering via gradually refined attention over appearance and motion},
	author    = {Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
	booktitle = MM,
	pages     = {1645--1653},
	publisher = {ACM},
	address   = {New York, NY, USA},
	year      = {2017}
}
@inproceedings{jiang2020divide,
	title     = {Divide and conquer: Question-guided spatio-temporal contextual attention for video question answering},
	author    = {Jiang, Jianwen and Chen, Ziqiang and Lin, Haojie and Zhao, Xibin and Gao, Yue},
	booktitle = AAAI,
	publisher = {The {AAAI} Press},
	address   = {New York, NY, USA},
	pages     = {11101--11108},
	year      = {2020}
}
@inproceedings{gao2018motion,
	title     = {Motion-appearance co-memory networks for video question answering},
	author    = {Gao, Jiyang and Ge, Runzhou and Chen, Kan and Nevatia, Ram},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Salt Lake City, UT, USA},
	pages     = {6576--6585},
	year      = {2018}
}
@article{zha2019spatiotemporal,
	title     = {Spatiotemporal-textual co-attention network for video question answering},
	author    = {Zha, Zheng-Jun and Liu, Jiawei and Yang, Tianhao and Zhang, Yongdong},
	journal   = TOMM,
	volume    = {15},
	number    = {2s},
	pages     = {1--18},
	year      = {2019},
	publisher = {ACM},
	address   = {New York, NY, USA}
}
@inproceedings{jin2019multi,
	title     = {Multi-interaction network with object relation for video question answering},
	author    = {Jin, Weike and Zhao, Zhou and Gu, Mao and Yu, Jun and Xiao, Jun and Zhuang, Yueting},
	booktitle = MM,
	publisher = {ACM},
	address   = {Nice, France},
	pages     = {1193--1201},
	year      = {2019}
}
@inproceedings{jiang2020reasoning,
	title     = {Reasoning with heterogeneous graph alignment for video question answering},
	author    = {Jiang, Pin and Han, Yahong},
	booktitle = AAAI,
	publisher = {The {AAAI} Press},
	address   = {New York, NY, USA},
	pages     = {11109--11116},
	year      = {2020}
}
@inproceedings{le2020hierarchical,
	title     = {Hierarchical conditional relation networks for video question answering},
	author    = {Le, Thao Minh and Le, Vuong and Venkatesh, Svetha and Tran, Truyen},
	booktitle = CVPR,
	publisher = {{IEEE}},
	address   = {Seattle, WA, USA},
	pages     = {9972--9981},
	year      = {2020}
}
@inproceedings{huang2020location,
	title     = {Location-aware graph convolutional networks for video question answering},
	author    = {Huang, Deng and Chen, Peihao and Zeng, Runhao and Du, Qing and Tan, Mingkui and Gan, Chuang},
	booktitle = AAAI,
	publisher = {The {AAAI} Press},
	address   = {New York, NY, USA},
	pages     = {11021--11028},
	year      = {2020}
}
@inproceedings{kim2020modality,
	title     = {Modality shifting attention network for multi-modal video question answering},
	author    = {Kim, Junyeong and Ma, Minuk and Pham, Trung and Kim, Kyungsu and Yoo, Chang D},
	booktitle = CVPR,
	publisher = {{IEEE}},
	address   = {Seattle, WA, USA},
	pages     = {10106--10115},
	year      = {2020}
}
@article{wang2021dualvgr,
	title     = {DualVGR: A dual-visual graph reasoning unit for video question answering},
	author    = {Wang, Jianyu and Bao, Bingkun and Xu, Changsheng},
	journal   = TMM,
	volume    = {24},
	pages     = {3369--3380},
	year      = {2022},
	publisher = {IEEE}
}
@inproceedings{seo2021attend,
	title     = {Attend what you need: Motion-appearance synergistic networks for video question answering},
	author    = {Seo, Ahjeong and Kang, Gi-Cheon and Park, Joonhan and Zhang, Byoung-Tak},
	booktitle = ACL,
	publisher = {ACL},
	address   = {Virtual},
	pages     = {6167--6177},
	year      = {2021}
}
@inproceedings{park2021bridge,
	title     = {Bridge to answer: Structure-aware graph interaction network for video question answering},
	author    = {Park, Jungin and Lee, Jiyoung and Sohn, Kwanghoon},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Virtual},
	pages     = {15526--15535},
	year      = {2021}
}
@inproceedings{yao2018exploring,
	title     = {Exploring visual relationship for image captioning},
	author    = {Yao, Ting and Pan, Yingwei and Li, Yehao and Mei, Tao},
	booktitle = ECCV,
	publisher = {Springer},
	address   = {Munich, Germany},
	pages     = {684--699},
	year      = {2018}
}
@inproceedings{cadene2019murel,
	title     = {{MUREL}: Multimodal relational reasoning for visual question answering},
	author    = {Cadene, Remi and Ben-Younes, Hedi and Cord, Matthieu and Thome, Nicolas},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Long Beach, CA, USA},
	pages     = {1989--1998},
	year      = {2019}
}
@inproceedings{li2019visual,
	title     = {Visual semantic reasoning for image-text matching},
	author    = {Li, Kunpeng and Zhang, Yulun and Li, Kai and Li, Yuanyuan and Fu, Yun},
	booktitle = ICCV,
	publisher = {IEEE},
	address   = {Seoul, Korea},
	pages     = {4654--4662},
	year      = {2019}
}
@inproceedings{yang2020prior,
	title     = {Prior visual relationship reasoning for visual question answering},
	author    = {Yang, Zhuoqian and Qin, Zengchang and Yu, Jing and Wan, Tao},
	booktitle = ICIP,
	publisher = {IEEE},
	address   = {Abu Dhabi, United Arab Emirates},
	pages     = {1411--1415},
	year      = {2020}
}
@inproceedings{pei2020visual,
	title     = {Visual relational reasoning for image caption},
	author    = {Pei, Haolei and Chen, Qiaohong and Wang, Ji and Sun, Qi and Jia, Yubo},
	booktitle = {International Joint Conference on Neural Networks},
	publisher = {IEEE},
	address   = {Glasgow, United Kingdom},
	pages     = {1--8},
	year      = {2020}
}
@inproceedings{chen2020figure,
	title     = {Figure captioning with relation maps for reasoning},
	author    = {Chen, Charles and Zhang, Ruiyi and Koh, Eunyee and Kim, Sungchul and Cohen, Scott and Rossi, Ryan},
	booktitle = WACV,
	publisher = {IEEE},
	address   = {Snowmass Village, CO, USA},
	pages     = {1537--1545},
	year      = {2020}
}
@inproceedings{li2019relation,
	title     = {Relation-aware graph attention network for visual question answering},
	author    = {Li, Linjie and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
	booktitle = ICCV,
	publisher = {IEEE},
	address   = {Seoul, Korea},
	pages     = {10313--10322},
	year      = {2019}
}
@inproceedings{chen2019graph,
	title     = {Graph-based global reasoning networks},
	author    = {Chen, Yunpeng and Rohrbach, Marcus and Yan, Zhicheng and Shuicheng, Yan and Feng, Jiashi and Kalantidis, Yannis},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Long Beach, CA, USA},
	pages     = {433--442},
	year      = {2019}
}
@inproceedings{gao2020multi,
	title     = {Multi-modal graph neural network for joint reasoning on vision and scene text},
	author    = {Gao, Difei and Li, Ke and Wang, Ruiping and Shan, Shiguang and Chen, Xilin},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Seattle, WA, USA},
	pages     = {12746--12756},
	year      = {2020}
}
@article{garcez2019neural,
	title   = {Neural-symbolic computing: An effective methodology for principled integration of machine learning and reasoning},
	author  = {Garcez, Artur d'Avila and Gori, Marco and Lamb, Luis C and Serafini, Luciano and Spranger, Michael and Tran, Son N},
	journal = {arXiv preprint arXiv:1905.06088},
	year    = {2019}
}
@inproceedings{vedantam2019probabilistic,
	title     = {Probabilistic neural-symbolic models for interpretable visual question answering},
	author    = {Vedantam, Ramakrishna and Desai, Karan and Lee, Stefan and Rohrbach, Marcus and Batra, Dhruv and Parikh, Devi},
	booktitle = ICML,
	address   = {Long Beach, CA, USA},
	publisher = {ACM},
	pages     = {6428--6437},
	year      = {2019}
}
@inproceedings{amizadeh2020neuro,
	title     = {Neuro-symbolic visual reasoning: Disentangling ``visual'' from ``reasoning''},
	author    = {Amizadeh, Saeed and Palangi, Hamid and Polozov, Oleksandr and Huang, Yichen and Koishida, Kazuhito},
	booktitle = ICML,
	address   = {Virtual},
	publisher = {ACM},
	pages     = {279--290},
	year      = {2020}
}
@inproceedings{norcliffe2018learning,
	title     = {Learning conditioned graph structures for interpretable visual question answering},
	author    = {Norcliffe-Brown, Will and Vafeias, Stathis and Parisot, Sarah},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Montreal, QC, Canada},
	pages     = {8334--8343},
	year      = {2018}
}
@article{shi2019simple,
	title   = {Simple {BERT} models for relation extraction and semantic role labeling},
	author  = {Shi, Peng and Lin, Jimmy},
	journal = {arXiv preprint arXiv:1904.05255},
	year    = {2019}
}
@article{hochreiter1997long,
	title     = {Long short-term memory},
	author    = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	journal   = {Neural Computation},
	volume    = {9},
	number    = {8},
	pages     = {1735--1780},
	year      = {1997},
	publisher = {The MIT Press}
}
@inproceedings{vaswani2017attention,
	title     = {Attention is all you need},
	author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Long Beach, CA, USA},
	pages     = {5998--6008},
	year      = {2017}
}
@inproceedings{garcia2020knowit,
	author    = {Noa Garcia and Mayu Otani and Chenhui Chu and Yuta Nakashima},
	title     = {{KnowIT VQA}: Answering knowledge-based questions about videos},
	booktitle = AAAI,
	publisher = {The {AAAI} Press},
	address   = {New York, NY, USA},
	pages     = {10826--10834},
	year      = {2020}
}
@inproceedings{lei2018tvqa,
	title     = {{TVQA}: Localized, compositional video question answering},
	author    = {Lei, Jie and Yu, Licheng and Bansal, Mohit and Berg, Tamara},
	booktitle = EMNLP,
	publisher = {ACL},
	address   = {Brussels, Belgium},
	pages     = {1369--1379},
	year      = {2018}
}
@inproceedings{he2016deep,
	title     = {Deep residual learning for Image recognition},
	author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Las Vegas, NV, USA},
	pages     = {770--778},
	year      = {2016}
}
@inproceedings{deng2009imagenet,
	title     = {{ImageNet}: A large-scale hierarchical image database},
	author    = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	booktitle = ICCV,
	publisher = {IEEE},
	address   = {Miami, FL, USA},
	pages     = {248--255},
	year      = {2009}
}
@article{krishna2017visual,
	title   = {{Visual Genome}: Connecting language and vision using crowdsourced dense image annotations},
	author  = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
	journal = IJCV,
	pages   = {32--73},
	volume  = {123},
	number  = {1},
	year    = {2017}
}
@inproceedings{anderson2018bottom,
	title     = {Bottom-up and top-down attention for image captioning and visual question answering},
	author    = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Salt Lake City, UT, USA},
	pages     = {6077--6086},
	year      = {2018}
}
@inproceedings{loshchilov2017decoupled,
	title     = {Decoupled weight decay regularization},
	author    = {Loshchilov, Ilya and Hutter, Frank},
	booktitle = ICLR,
	publisher = {Openreview},
	address   = {New Orleans, LA, USA},
	year      = {2019}
}
@inproceedings{fan2019heterogeneous,
	title     = {Heterogeneous memory enhanced multimodal attention model for video question answering},
	author    = {Fan, Chenyou and Zhang, Xiaofan and Zhang, Shu and Wang, Wensheng and Zhang, Chi and Huang, Heng},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Long Beach, CA, USA},
	pages     = {1999--2007},
	year      = {2019}
}
@inproceedings{jang2017tgif,
	title     = {{TGIF-QA}: Toward spatio-temporal reasoning in visual question answering},
	author    = {Jang, Yunseok and Song, Yale and Yu, Youngjae and Kim, Youngjin and Kim, Gunhee},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Honolulu, HI, USA},
	pages     = {2758--2766},
	year      = {2017}
}
@inproceedings{yang2021just,
	title     = {Just ask: Learning to answer questions from millions of narrated videos},
	author    = {Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
	booktitle = ICCV,
	publisher = {IEEE},
	address   = {Montreal, QC, Canada},
	pages     = {1686--1697},
	year      = {2021}
}
@inproceedings{seo2021look,
	title     = {Look before you speak: Visually contextualized utterances},
	author    = {Seo, Paul Hongsuck and Nagrani, Arsha and Schmid, Cordelia},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Virtual},
	pages     = {16877--16887},
	year      = {2021}
}
@inproceedings{lei2021less,
	title     = {Less is more: Clipbert for video-and-language learning via sparse sampling},
	author    = {Lei, Jie and Li, Linjie and Zhou, Luowei and Gan, Zhe and Berg, Tamara L and Bansal, Mohit and Liu, Jingjing},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Virtual},
	pages     = {7331--7341},
	year      = {2021}
}
@inproceedings{amrani2021noise,
	title     = {Noise estimation using density estimation for self-Supervised multimodal learning},
	author    = {Amrani, Elad and Ben-Ari, Rami and Rotman, Daniel and Bronstein, Alex},
	booktitle = AAAI,
	publisher = {The {AAAI} Press},
	address   = {Virtual},
	pages     = {6644--6652},
	year      = {2021}
}
@inproceedings{garcia2020knowledge,
	title     = {Knowledge-based video question answering with unsupervised scene descriptions},
	author    = {Garcia, Noa and Nakashima, Yuta},
	booktitle = ECCV,
	publisher = {Springer},
	address   = {Glasgow, UK},
	pages     = {581--598},
	year      = {2020}
}
@inproceedings{parkhi2015deep,
	title     = {Deep face recognition},
	author    = {Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew},
	booktitle = BMVC,
	publisher = {BMVA},
	address   = {Swansea, UK},
	pages     = {41.1--41.12},
	year      = {2015}
}
@inproceedings{xu2015show,
	title     = {Show, attend and tell: Neural image caption generation with visual attention},
	author    = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
	booktitle = ICML,
	publisher = {ACM},
	address   = {Lille, France},
	pages     = {2048--2057},
	year      = {2015}
}
@inproceedings{kim2019progressive,
	title     = {Progressive attention memory network for movie story question answering},
	author    = {Kim, Junyeong and Ma, Minuk and Kim, Kyungsu and Kim, Sungjin and Yoo, Chang D},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Long Beach, CA, USA},
	pages     = {8337--8346},
	year      = {2019}
}
@inproceedings{lei2020tvqa,
	title     = {{TVQA}+: Spatio-temporal grounding for video question answering},
	author    = {Lei, Jie and Yu, Licheng and Berg, Tamara L and Bansal, Mohit},
	booktitle = ACL,
	publisher = {ACL},
	address   = {Virtual},
	pages     = {8211--8225},
	year      = {2020}
}
@inproceedings{zhu2020mucko,
	title     = {Mucko: Multi-layer cross-modal knowledge reasoning for fact-based visual question answering},
	author    = {Zhu, Zihao and Yu, Jing and Wang, Yujing and Sun, Yajing and Hu, Yue and Wu, Qi},
	booktitle = IJCAI,
	publisher = {Morgan Kaufmann},
	address   = {Yokohama, Japan},
	pages     = {1097--1103},
	year      = {2020}
}
@article{scarselli2008graph,
	title     = {The graph neural network model},
	author    = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
	journal   = {{IEEE} Transactions on Neural Networks},
	volume    = {20},
	number    = {1},
	pages     = {61--80},
	year      = {2009},
	publisher = {IEEE}
}
@inproceedings{zhang2018end,
	title     = {An end-to-end deep learning architecture for graph classification},
	author    = {Zhang, Muhan and Cui, Zhicheng and Neumann, Marion and Chen, Yixin},
	booktitle = AAAI,
	publisher = {The {AAAI} Press},
	address   = {New Orleans, LA, USA},
	pages     = {4438--4445},
	year      = {2018}
}
@article{bai2020learning,
	title     = {Learning backtrackless aligned-spatial graph convolutional networks for graph classification},
	author    = {Bai, Lu and Cui, Lixin and Jiao, Yuhang and Rossi, Luca and Hancock, Edwin},
	journal   = TPAMI,
	volume    = {44},
	number    = {2},
	pages     = {783--798},
	year      = {2020},
	publisher = {IEEE}
}
@article{wang2020learning,
	title     = {Learning coarse-to-fine graph neural networks for video-text retrieval},
	author    = {Wang, Wei and Gao, Junyu and Yang, Xiaoshan and Xu, Changsheng},
	journal   = TMM,
	volume    = {23},
	pages     = {2386--2397},
	year      = {2021},
	publisher = {IEEE}
}
@article{song2021spatial,
	title     = {Spatial-temporal graphs for cross-modal text2video retrieval},
	author    = {Song, Xue and Chen, Jingjing and Wu, Zuxuan and Jiang, Yu-Gang},
	journal   = TMM,
	volume    = {24},
	pages     = {2914--2923},
	year      = {2021},
	publisher = {IEEE}
}
@inproceedings{jiang2021x,
	title     = {{X-GGM}: Graph generative modeling for out-of-distribution generalization in visual question answering},
	author    = {Jiang, Jingjing and Liu, Ziyi and Liu, Yifan and Nan, Zhixiong and Zheng, Nanning},
	booktitle = MM,
	publisher = {ACM},
	address   = {Virtual},
	pages     = {199--208},
	year      = {2021}
}
@inproceedings{bruna2013spectral,
	title     = {Spectral networks and locally connected networks on graphs},
	author    = {Bruna, Joan and Zaremba, Wojciech and Szlam, Arthur and LeCun, Yann},
	booktitle = ICLR,
	publisher = {Openreview},
	address   = {Banff, AB, Canada},
	year      = {2014}
}
@article{henaff2015deep,
	title   = {Deep convolutional networks on graph-structured data},
	author  = {Henaff, Mikael and Bruna, Joan and LeCun, Yann},
	journal = {arXiv preprint arXiv:1506.05163},
	year    = {2015}
}
@inproceedings{rippel2015spectral,
	title     = {Spectral representations for convolutional neural networks},
	author    = {Rippel, Oren and Snoek, Jasper and Adams, Ryan P},
	pages     = {2449--2457},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Montreal, QC, Canada},
	year      = {2015}
}
@inproceedings{defferrard2016convolutional,
	title     = {Convolutional neural networks on graphs with fast localized spectral filtering},
	author    = {Defferrard, Micha{\"e}l and Bresson, Xavier and Vandergheynst, Pierre},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Barcel- ona, Spain},
	pages     = {3844--3852},
	year      = {2016}
}
@inproceedings{kipf2016semi,
	title     = {Semi-supervised classification with graph convolutional networks},
	author    = {Kipf, Thomas N and Welling, Max},
	booktitle = ICLR,
	publisher = {Openreview},
	address   = {Toulon, France},
	year      = {2017}
}
@inproceedings{velivckovic2017graph,
	title     = {Graph attention networks},
	author    = {Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
	booktitle = ICLR,
	publisher = {Openreview},
	address   = {Vancouver, BC, Canada},
	year      = {2018}
}
@inproceedings{hamilton2017inductive,
	title     = {Inductive representation learning on large graphs},
	author    = {Hamilton, William L and Ying, Rex and Leskovec, Jure},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Long Beach, CA, USA},
	pages     = {1025--1035},
	year      = {2017}
}
@inproceedings{antol2015vqa,
	title     = {Vqa: Visual question answering},
	author    = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Lawrence Zitnick, C and Parikh, Devi},
	booktitle = ICCV,
	publisher = {{IEEE}},
	address   = {Santiago, Chile},
	pages     = {2425--2433},
	year      = {2015}
}
@inproceedings{kervadec2021roses,
	title     = {Roses are red, violets are blue... but should vqa expect them to?},
	author    = {Kervadec, Corentin and Antipov, Grigory and Baccouche, Moez and Wolf, Christian},
	booktitle = CVPR,
	publisher = {{IEEE}},
	address   = {Virtual},
	pages     = {2776--2785},
	year      = {2021}
}
@inproceedings{teney2020value,
	title     = {On the value of out-of-distribution testing: An example of goodhart's law},
	author    = {Teney, Damien and Kafle, Kushal and Shrestha, Robik and Abbasnejad, Ehsan and Kanan, Christopher and Hengel, Anton van den},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Virtual},
	pages     = {407--417},
	year      = {2020}
}
@inproceedings{agrawal2016analyzing,
	title     = {Analyzing the behavior of visual question answering models},
	author    = {Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi},
	booktitle = EMNLP,
	publisher = {ACL},
	address   = {Austin, TX, USA},
	pages     = {1955--1960},
	year      = {2016}
}
@inproceedings{zhang2016yin,
	title     = {Yin and yang: Balancing and answering binary visual questions},
	author    = {Zhang, Peng and Goyal, Yash and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Las Vegas, NV, USA},
	pages     = {5014--5022},
	year      = {2016}
}
@inproceedings{goyal2017making,
	title     = {Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
	author    = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Honolulu, HI, USA},
	pages     = {6904--6913},
	year      = {2017}
}
@inproceedings{agrawal2018don,
	title     = {Don't just assume; look and answer: Overcoming priors for visual question answering},
	author    = {Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi and Kembhavi, Aniruddha},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Salt Lake City, UT, USA},
	pages     = {4971--4980},
	year      = {2018}
}
@inproceedings{clark2019don,
	title     = {Don't take the easy way out: Ensemble based methods for avoiding known dataset biases},
	author    = {Christopher Clark and Mark Yatskar and Luke Zettlemoyer},
	booktitle = EMNLP,
	publisher = {ACL},
	address   = {Hong Kong, China},
	pages     = {4067--4080},
	year      = {2019}
}
@inproceedings{li2022mplug,
	title     = {mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections},
	author    = {Li, Chenliang and Xu, Haiyang and Tian, Junfeng and Wang, Wei and Yan, Ming and Bi, Bin and Ye, Jiabo and Chen, Hehong and Xu, Guohai and Cao, Zheng and others},
	booktitle = EMNLP,
	publisher = {ACL},
	pages     = {7241--7259},
	year      = {2022}
}
@inproceedings{wu2019self,
	title     = {Self-critical reasoning for robust visual question answering},
	author    = {Wu, Jialin and Mooney, Raymond},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Vancouver, BC, Canada},
	pages     = {8604--8614},
	year      = {2019}
}
@inproceedings{gokhale2020mutant,
	title     = {Mutant: A training paradigm for out-of-distribution generalization in visual question answering},
	author    = {Gokhale, Tejas and Banerjee, Pratyay and Baral, Chitta and Yang, Yezhou},
	booktitle = EMNLP,
	publisher = {ACL},
	address   = {Virtual},
	pages     = {878--892},
	year      = {2020}
}
@inproceedings{clark2020learning,
	title     = {Learning to model and ignore dataset bias with mixed capacity ensembles},
	author    = {Clark, Christopher and Yatskar, Mark and Zettlemoyer, Luke},
	booktitle = EMNLP,
	publisher = {ACL},
	address   = {Virtual},
	pages     = {3031--3045},
	year      = {2020}
}
@article{lake2017building,
	title     = {Building machines that learn and think like people},
	author    = {Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
	journal   = {Behavioral and Brain Sciences},
	volume    = {40},
	year      = {2017},
	publisher = {Cambridge University Press}
}
@inproceedings{wang2018graphgan,
	title     = {Graphgan: Graph representation learning with generative adversarial nets},
	author    = {Wang, Hongwei and Wang, Jia and Wang, Jialin and Zhao, Miao and Zhang, Weinan and Zhang, Fuzheng and Xie, Xing and Guo, Minyi},
	booktitle = AAAI,
	publisher = {The {AAAI} Press},
	address   = {New Orleans, LA, USA},
	pages     = {2508--2515},
	year      = {2018}
}
@inproceedings{dai2018adversarial,
	title     = {Adversarial network embedding},
	author    = {Dai, Quanyu and Li, Qiang and Tang, Jian and Wang, Dan},
	booktitle = AAAI,
	publisher = {The {AAAI} Press},
	address   = {New Orleans, LA, USA},
	pages     = {2167--2174},
	year      = {2018}
}
@inproceedings{zheng2020distribution,
	title     = {Distribution-induced bidirectional generative adversarial network for graph representation learning},
	author    = {Zheng, Shuai and Zhu, Zhenfeng and Zhang, Xingxing and Liu, Zhizhe and Cheng, Jian and Zhao, Yao},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Seattle, WA, USA},
	pages     = {7224--7233},
	year      = {2020}
}
@inproceedings{guo2019quantifying,
	title     = {Quantifying and alleviating the language prior problem in visual question answering},
	author    = {Guo, Yangyang and Cheng, Zhiyong and Nie, Liqiang and Liu, Yibing and Wang, Yinglong and Kankanhalli, Mohan},
	booktitle = SIGIR,
	publisher = {ACM},
	address   = {Paris, France},
	pages     = {75--84},
	year      = {2019}
}
@inproceedings{chen2020counterfactual,
	title     = {Counterfactual samples synthesizing for robust visual question answering},
	author    = {Chen, Long and Yan, Xin and Xiao, Jun and Zhang, Hanwang and Pu, Shiliang and Zhuang, Yueting},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Seattle, WA, USA},
	pages     = {10800--10809},
	year      = {2020}
}
@inproceedings{liang2020learning,
	title     = {Learning to contrast the counterfactual samples for robust visual question answering},
	author    = {Liang, Zujie and Jiang, Weitao and Hu, Haifeng and Zhu, Jiaying},
	booktitle = EMNLP,
	publisher = {ACL},
	address   = {Virtual},
	pages     = {3285--3292},
	year      = {2020}
}
@inproceedings{teney2020unshuffling,
	title     = {Unshuffling data for improved generalization in visual question answering},
	author    = {Teney, Damien and Abbasnejad, Ehsan and Hengel, Anton van den},
	booktitle = ICCV,
	publisher = {IEEE},
	address   = {Montreal, QC, Canada},
	pages     = {1417--1427},
	year      = {2021}
}
@inproceedings{selvaraju2019taking,
	title     = {Taking a hint: Leveraging explanations to make vision and language models more grounded},
	author    = {Selvaraju, Ramprasaath R and Lee, Stefan and Shen, Yilin and Jin, Hongxia and Ghosh, Shalini and Heck, Larry and Batra, Dhruv and Parikh, Devi},
	booktitle = ICCV,
	publisher = {IEEE},
	address   = {Seoul, Korea},
	pages     = {2591--2600},
	year      = {2019}
}
@article{grand2019adversarial,
	title   = {Adversarial regularization for visual question answering: Strengths, shortcomings, and side effects},
	author  = {Grand, Gabriel and Belinkov, Yonatan},
	journal = {arXiv preprint arXiv:1906.08430},
	year    = {2019}
}
@inproceedings{teney2020learning,
	title     = {Learning what makes a difference from counterfactual examples and gradient supervision},
	author    = {Damien Teney and Ehsan Abbasnejad and A. V. D. Hengel},
	booktitle = ECCV,
	publisher = {Springer},
	address   = {Glasgow, UK},
	pages     = {580--599},
	year      = {2020}
}
@article{li2020closer,
	title   = {A closer look at the robustness of vision-and-language pre-trained models},
	author  = {Li, Linjie and Gan, Zhe and Liu, Jingjing},
	journal = {arXiv preprint arXiv:2012.08673},
	year    = {2020}
}
@inproceedings{gan2020large,
	title     = {Large-scale adversarial training for vision-and-language representation learning},
	author    = {Gan, Zhe and Chen, Yen-Chun and Li, Linjie and Zhu, Chen and Cheng, Yu and Liu, Jingjing},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Virtual},
	pages     = {6616-6628},
	year      = {2020}
}
@article{de2018molgan,
	title   = {MolGAN: An implicit generative model for small molecular graphs},
	author  = {De Cao, Nicola and Kipf, Thomas},
	journal = {arXiv preprint arXiv:1805.11973},
	year    = {2018}
}
@inproceedings{nauata2020house,
	title     = {House-GAN: Relational generative adversarial networks for graph-constrained house layout generation},
	author    = {Nauata, Nelson and Chang, Kai-Hung and Cheng, Chin-Yi and Mori, Greg and Furukawa, Yasutaka},
	booktitle = ECCV,
	publisher = {Springer},
	address   = {Glasgow, UK},
	pages     = {162--177},
	year      = {2020}
}
@inproceedings{yu2018learning,
	title     = {Learning deep network representations with adversarially regularized autoencoders},
	author    = {Yu, Wenchao and Zheng, Cheng and Cheng, Wei and Aggarwal, Charu C and Song, Dongjin and Zong, Bo and Chen, Haifeng and Wang, Wei},
	booktitle = KDD,
	publisher = {ACM},
	address   = {London, UK},
	pages     = {2663--2671},
	year      = {2018}
}
@inproceedings{pan2018adversarially,
	title     = {Adversarially regularized graph autoencoder for graph embedding},
	author    = {Pan, Shirui and Hu, Ruiqi and Long, Guodong and Jiang, Jing and Yao, Lina and Zhang, Chengqi},
	booktitle = IJCAI,
	publisher = {Morgan Kaufmann},
	address   = {Stockholm, Sweden},
	pages     = {2609--2615},
	year      = {2018}
}
@inproceedings{kingma2013auto,
	title     = {Auto-encoding variational bayes},
	author    = {Kingma, Diederik P and Welling, Max},
	booktitle = ICLR,
	publisher = {Openreview},
	address   = {Banff, AB, Canada},
	year      = {2014}
}
@article{goodfellow2014generative,
	title   = {Generative adversarial networks},
	author  = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	journal = {arXiv preprint arXiv:1406.2661},
	year    = {2014}
}
@article{kipf2016variational,
	title   = {Variational graph auto-encoders},
	author  = {Kipf, Thomas N and Welling, Max},
	journal = {arXiv preprint arXiv:1611.07308},
	year    = {2016}
}
@inproceedings{simonovsky2018graphvae,
	title     = {Graphvae: Towards generation of small graphs using variational autoencoders},
	author    = {Simonovsky, Martin and Komodakis, Nikos},
	booktitle = {International Conference on Artificial Neural Networks},
	publisher = {Springer},
	address   = {Rhodes, Greece},
	pages     = {412--422},
	year      = {2018}
}
@inproceedings{grover2019graphite,
	title     = {Graphite: Iterative generative modeling of graphs},
	author    = {Grover, Aditya and Zweig, Aaron and Ermon, Stefano},
	booktitle = {Internat- ional Conference on Machine Learning},
	address   = {Long Beach, CA, USA},
	publisher = {ACM},
	pages     = {2434--2444},
	year      = {2019}
}
@inproceedings{you2018graphrnn,
	title     = {Graphrnn: Generating realistic graphs with deep auto-regressive models},
	author    = {You, Jiaxuan and Ying, Rex and Ren, Xiang and Hamilton, William L and Leskovec, Jure},
	booktitle = ICML,
	publisher = {ACM},
	address   = {Stockholmsm{\"{a}}ssan, Stockholm, Sweden},
	pages     = {5694--5703},
	year      = {2018}
}
@inproceedings{liao2019efficient,
	title     = {Efficient graph generation with graph recurrent attention networks},
	author    = {Liao, Renjie and Li, Yujia and Song, Yang and Wang, Shenlong and Nash, Charlie and Hamilton, William L and Duvenaud, David and Urtasun, Raquel and Zemel, Richard S},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Vancouver, BC, Canada},
	pages     = {4257--4267},
	year      = {2019}
}
@inproceedings{shi2020graphaf,
	title     = {{GraphAF}: A flow-based autoregressive model for molecular graph generation},
	author    = {Shi, Chence and Xu, Minkai and Zhu, Zhaocheng and Zhang, Weinan and Zhang, Ming and Tang, Jian},
	booktitle = ICLR,
	publisher = {Openreview},
	address   = {Addis Ababa, Ethiopia},
	year      = {2020}
}
@article{kawai2019scalable,
	title   = {Scalable generative models for graphs with graph attention mechanism},
	author  = {Kawai, Wataru and Mukuta, Yusuke and Harada, Tatsuya},
	journal = {arXiv preprint arXiv:1906.01861},
	year    = {2019}
}
@inproceedings{you2020graph,
	title      = {Graph contrastive learning with augmentations},
	author     = {You, Yuning and Chen, Tianlong and Sui, Yongduo and Chen, Ting and Wang, Zhangyang and Shen, Yang},
	booktitle = NIPS,
	publisher  = {The MIT Press},
	address    = {Virtual},
	year       = {2020}
}
@inproceedings{hu2019language,
	title     = {Language-conditioned graph networks for relational reasoning},
	author    = {Hu, Ronghang and Rohrbach, Anna and Darrell, Trevor and Saenko, Kate},
	booktitle = ICCV,
	publisher = {IEEE},
	address   = {Seoul, Korea},
	pages     = {10294--10303},
	year      = {2019}
}
@inproceedings{yao2018exploring,
	title     = {Exploring visual relationship for image captioning},
	author    = {Yao, Ting and Pan, Yingwei and Li, Yehao and Mei, Tao},
	booktitle = ECCV,
	publisher = {Springer},
	address   = {Munich, Germany},
	pages     = {684--699},
	year      = {2018}
}
@inproceedings{guo2019aligning,
	title     = {Aligning linguistic words and visual semantic units for image captioning},
	author    = {Guo, Longteng and Liu, Jing and Tang, Jinhui and Li, Jiangwei and Luo, Wei and Lu, Hanqing},
	booktitle = MM,
	publisher = {ACM},
	address   = {Nice, France},
	pages     = {765--773},
	year      = {2019}
}
@inproceedings{zhang2021consensus,
	title     = {Consensus graph representation learning for better grounded image captioning},
	author    = {Zhang, Wenqiao and Shi, Haochen and Tang, Siliang and Xiao, Jun and Yu, Qiang and Zhuang, Yueting},
	booktitle = AAAI,
	publisher = {The {AAAI} Press},
	address   = {Virtual},
	pages     = {3394--3402},
	year      = {2021}
}
@inproceedings{jing2020visual,
	title     = {Visual-semantic graph matching for visual grounding},
	author    = {Jing, Chenchen and Wu, Yuwei and Pei, Mingtao and Hu, Yao and Jia, Yunde and Wu, Qi},
	booktitle = MM,
	publisher = {ACM},
	address   = {Seattle, WA, USA},
	pages     = {4041--4050},
	year      = {2020}
}
@inproceedings{liu2020learning,
	title     = {Learning cross-modal context graph for visual grounding},
	author    = {Liu, Yongfei and Wan, Bo and Zhu, Xiaodan and He, Xuming},
	booktitle = AAAI,
	publisher = {The {AAAI} Press},
	address   = {New York, NY, USA},
	pages     = {11645--11652},
	year      = {2020}
}
@inproceedings{yang2020graph,
	title     = {Graph-structured referring expression reasoning in the wild},
	author    = {Yang, Sibei and Li, Guanbin and Yu, Yizhou},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Seattle, WA, USA},
	pages     = {9952--9961},
	year      = {2020}
}
@inproceedings{kant2020spatially,
	title     = {Spatially aware multimodal transformers for textvqa},
	author    = {Kant, Yash and Batra, Dhruv and Anderson, Peter and Schwing, Alexander and Parikh, Devi and Lu, Jiasen and Agrawal, Harsh},
	booktitle = ECCV,
	publisher = {Springer},
	address   = {Glasgow, UK},
	pages     = {715--732},
	year      = {2020}
}
@inproceedings{tan2019lxmert,
	title     = {{LXMERT}: Learning cross-modality encoder representations from transformers},
	author    = {Tan, Hao and Bansal, Mohit},
	booktitle = EMNLP,
	publisher = {ACL},
	address   = {Hong Kong, China},
	pages     = {5099--5110},
	year      = {2019}
}
@inproceedings{devlin2018bert,
	title     = {{BERT:} Pre-training of deep bidirectional transformers for language understanding},
	author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	booktitle = ACL,
	publisher = {ACL},
	address   = {Minneapolis, MN, USA},
	pages     = {4171--4186},
	year      = {2019}
}
@inproceedings{vincent2008extracting,
	title     = {Extracting and composing robust features with denoising autoencoders},
	author    = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	booktitle = ICML,
	publisher = {ACM},
	address   = {Helsinki, Finland},
	pages     = {1096--1103},
	year      = {2008}
}
@article{bigdeli2020learning,
	title   = {Learning generative models using denoising density estimators},
	author  = {Bigdeli, Siavash A and Lin, Geng and Portenier, Tiziano and Dunbar, L Andrea and Zwicker, Matthias},
	journal = {arXiv preprint arXiv:2001.02728},
	year    = {2020}
}
@inproceedings{shih2016look,
	title     = {Where to look: Focus regions for visual question answering},
	author    = {Shih, Kevin J and Singh, Saurabh and Hoiem, Derek},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Las Vegas, NV, USA},
	pages     = {4613--4621},
	year      = {2016}
}
@inproceedings{hudson2019gqa,
	title     = {{GQA}: A new dataset for real-world visual reasoning and compositional question answering},
	author    = {Hudson, Drew A and Manning, Christopher D},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Long Beach, CA, USA},
	pages     = {6700--6709},
	year      = {2019}
}
@inproceedings{teney2019actively,
	title     = {Actively seeking and learning from live data},
	author    = {Teney, Damien and Hengel, Anton van den},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Long Beach, CA, USA},
	pages     = {1940--1949},
	year      = {2019}
}
@inproceedings{gat2020removing,
	title     = {Removing bias in multi-modal classifiers: Regularization by maximizing functional entropies},
	author    = {Gat, Itai and Schwartz, Idan and Schwing, Alexander and Hazan, Tamir},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Virtual},
	pages     = {3197-3208},
	year      = {2020}
}
@article{guo2021loss,
	title     = {Loss re-scaling VQA: Revisiting the language prior problem from a class-imbalance view},
	author    = {Guo, Yangyang and Nie, Liqiang and Cheng, Zhiyong and Tian, Qi and Zhang, Min},
	journal   = TMM,
	volume    = {31},
	pages     = {227--238},
	year      = {2021},
	publisher = {IEEE}
}
@inproceedings{kv2020reducing,
	title     = {Reducing language biases in visual question answering with visually-grounded question encoder},
	author    = {KV, Gouthaman and Mittal, Anurag},
	booktitle = ECCV,
	publisher = {Springer},
	address   = {Glasgow, UK},
	pages     = {18--34},
	year      = {2020}
}
@inproceedings{cadene2019rubi,
	title     = {{RUBi}: Reducing unimodal biases for visual question answering},
	author    = {Cadene, Remi and Dancette, Corentin and Cord, Matthieu and Parikh, Devi and others},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Vancouver, BC, Canada},
	pages     = {841--852},
	year      = {2019}
}
@inproceedings{niu2021counterfactual,
	title     = {Counterfactual vqa: A cause-effect look at language bias},
	author    = {Niu, Yulei and Tang, Kaihua and Zhang, Hanwang and Lu, Zhiwu and Hua, Xian-Sheng and Wen, Ji-Rong},
	booktitle = CVPR,
	publisher = {{IEEE}},
	address   = {Virtual},
	pages     = {12700--12710},
	year      = {2021}
}
@inproceedings{jing2020overcoming,
	author    = {Chenchen Jing and Yuwei Wu and Xiaoxun Zhang and Yunde Jia and Qi Wu},
	title     = {Overcoming language priors in {VQA} via decomposed linguistic representations},
	booktitle = AAAI,
	publisher = {The {AAAI} Press},
	address   = {New York, NY, USA},
	pages     = {11181--11188},
	year      = {2020}
}
@inproceedings{zhu2020overcoming,
	title     = {Overcoming language priors with self-supervised learning for visual question answering},
	author    = {Zhu, Xi and Mao, Zhendong and Liu, Chunxiao and Zhang, Peng and Wang, Bin and Zhang, Yongdong},
	booktitle = IJCAI,
	publisher = {Morgan Kaufmann},
	address   = {Yokohama, Japan},
	pages     = {1083--1089},
	year      = {2020}
}
@inproceedings{qiao2020rankvqa,
	title     = {Rankvqa: Answer re-ranking for visual question answering},
	author    = {Qiao, Yanyuan and Yu, Zheng and Liu, Jing},
	booktitle = ICME,
	publisher = {{IEEE}},
	address   = {London, UK},
	pages     = {1--6},
	year      = {2020}
}
@inproceedings{yang2021learning,
	title     = {Learning content and context with language bias for visual question answering},
	author    = {Yang, Chao and Feng, Su and Li, Dongsheng and Shen, Huawei and Wang, Guoqing and Jiang, Bin},
	booktitle = ICME,
	publisher = {{IEEE}},
	address   = {Shenzhen, China},
	pages     = {1--6},
	year      = {2021}
}
@inproceedings{kim2018bilinear,
	title     = {Bilinear attention networks},
	author    = {Kim, Jin-Hwa and Jun, Jaehyun and Zhang, Byoung-Tak},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Montral, QC, Canada},
	pages     = {1564--1574},
	year      = {2018}
}
@inproceedings{chen2021meta,
	title     = {Meta module network for compositional visual reasoning},
	author    = {Chen, Wenhu and Gan, Zhe and Li, Linjie and Cheng, Yu and Wang, William and Liu, Jingjing},
	booktitle = WACV,
	publisher = {IEEE},
	address   = {Waikoloa, HI, USA},
	pages     = {655--664},
	year      = {2021}
}
@inproceedings{yu2019deep,
	title     = {Deep modular co-attention networks for visual question answering},
	author    = {Yu, Zhou and Yu, Jun and Cui, Yuhao and Tao, Dacheng and Tian, Qi},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Long Beach, CA, USA},
	pages     = {6281--6290},
	year      = {2019}
}
@inproceedings{xu2018powerful,
	title     = {How powerful are graph neural networks?},
	author    = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
	booktitle = ICLR,
	publisher = {Openreview},
	address   = {New Orleans, LA, USA},
	year      = {2019}
}
% CIB
@inproceedings{wang2021simvlm,
	title     = {SimVLM: Simple visual language model pretraining with weak supervision},
	author    = {Wang, Zirui and Yu, Jiahui and Yu, Adams Wei and Dai, Zihang and Tsvetkov, Yulia and Cao, Yuan},
	booktitle = ICLR,
	publisher = {Openreview},
	address   = {Virtual},
	year      = {2022}
}
@article{wang2021vlmo,
	title   = {VLMo: Unified vision-language pre-training with mixture-of-modality-experts},
	author  = {Wang, Wenhui and Bao, Hangbo and Dong, Li and Wei, Furu},
	journal = {arXiv preprint arXiv:2111.02358},
	year    = {2021}
}
@inproceedings{li2022blip,
	title   = {Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
	author  = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
	booktitle = ICML,
	publisher = {ACM},
	address   = {Baltimore, MD, USA},
	pages     = {12888--12900},
	year      = {2022}
}
@inproceedings{shah2019cycle,
	title     = {Cycle-consistency for robust visual question answering},
	author    = {Shah, Meet and Chen, Xinlei and Rohrbach, Marcus and Parikh, Devi},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Long Beach, CA, USA},
	pages     = {6649--6658},
	year      = {2019}
}
@article{whitehead2020learning,
	author  = {Whitehead, Spencer and Wu, Hui and Fung, Yi Ren and Ji, Heng and Feris, Rogerio and Saenko, Kate},
	title   = {Learning from lexical perturbations for consistent visual question answering},
	journal = {arXiv preprint arXiv:2011.13406},
	year    = {2020}
}
@inproceedings{agarwal2020towards,
	title     = {Towards causal vqa: Revealing and reducing spurious correlations by invariant and covariant semantic editing},
	author    = {Agarwal, Vedika and Shetty, Rakshith and Fritz, Mario},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Seattle, WA, USA},
	pages     = {9690--9698},
	year      = {2020}
}
@inproceedings{li2021adversarial,
	title     = {Adversarial vqa: A new benchmark for evaluating the robustness of vqa models},
	author    = {Li, Linjie and Lei, Jie and Gan, Zhe and Liu, Jingjing},
	booktitle = ICCV,
	publisher = {IEEE},
	address   = {Montreal, QC, Canada},
	pages     = {2022--2031},
	year      = {2021}
}
@inproceedings{sheng2021human,
	title     = {Human-adversarial visual question answering},
	author    = {Sheng, Sasha and Singh, Amanpreet and Goswami, Vedanuj and Magana, Jose and Thrush, Tristan and Galuba, Wojciech and Parikh, Devi and Kiela, Douwe},
	booktitle = {Adva- nces in Neural Information Processing Systems},
	publisher = {The MIT Press},
	address   = {Virtual},
	pages     = {20346--20359},
	year      = {2021}
}
@article{tishby2000information,
	title   = {The information bottleneck method},
	author  = {Tishby, Naftali and Pereira, Fernando C and Bialek, William},
	journal = {arXiv preprint physics/0004057},
	year    = {2000}
}
@inproceedings{kant2021contrast,
	title     = {Contrast and classify: Training robust vqa models},
	author    = {Kant, Yash and Moudgil, Abhinav and Batra, Dhruv and Parikh, Devi and Agrawal, Harsh},
	booktitle = ICCV,
	publisher = {IEEE},
	address   = {Montreal, QC, Canada},
	pages     = {1604--1613},
	year      = {2021}
}
@inproceedings{tishby2015deep,
	title     = {Deep learning and the information bottleneck principle},
	author    = {Tishby, Naftali and Zaslavsky, Noga},
	booktitle = {IEEE Information Theory Workshop},
	publisher = {IEEE},
	address   = {Jerusalem, Israel},
	pages     = {1--5},
	year      = {2015}
}
@article{shwartz2017opening,
	title   = {Opening the black box of deep neural networks via information},
	author  = {Shwartz-Ziv, Ravid and Tishby, Naftali},
	journal = {arXiv preprint arXiv:1703.00810},
	year    = {2017}
}
@inproceedings{du2020learning,
	title     = {Learning to learn with variational information bottleneck for domain generalization},
	author    = {Du, Yingjun and Xu, Jun and Xiong, Huan and Qiu, Qiang and Zhen, Xiantong and Snoek, Cees GM and Shao, Ling},
	booktitle = ECCV,
	publisher = {Springer},
	address   = {Glasgow, UK},
	pages     = {200--216},
	year      = {2020}
}
@inproceedings{li2022invariant,
	title     = {Invariant information bottleneck for domain generalization},
	author    = {Li, Bo and Shen, Yifei and Wang, Yezhen and Zhu, Wenzhen and Li, Dongsheng and Keutzer, Kurt and Zhao, Han},
	booktitle = AAAI,
	publisher = {The {AAAI} Press},
	address   = {Virtual},
	pages     = {7399--7407},
	year      = {2022}
}
@inproceedings{wang2021infobert,
	title     = {{InfoBERT}: Improving robustness of language models from an information theoretic perspective},
	author    = {Wang, Boxin and Wang, Shuohang and Cheng, Yu and Gan, Zhe and Jia, Ruoxi and Li, Bo and Liu, Jingjing},
	booktitle = ICLR,
	publisher = {Openreview},
	address   = {Virtual},
	year      = {2021}
}
@inproceedings{federici2020learning,
	title     = {Learning robust representations via multi-view information bottleneck},
	author    = {Federici, Marco and Dutta, Anjan and Forr{\'e}, Patrick and Kushman, Nate and Akata, Zeynep},
	booktitle = ICLR,
	publisher = {Openreview},
	address   = {Addis Ababa, Ethiopia},
	year      = {2020}
}
@inproceedings{mahabadi2021variational,
	title     = {Variational information bottleneck for effective low-resource fine-tuning},
	author    = {Mahabadi, Rabeeh Karimi and Belinkov, Yonatan and Henderson, James},
	booktitle = ICLR,
	publisher = {Openreview},
	address   = {Virtual},
	year      = {2021}
}
@article{bao2021disentangled,
	title   = {Disentangled variational information bottleneck for multiview representation learning},
	author  = {Bao, Feng},
	journal = {arXiv preprint arXiv:2105.07599},
	year    = {2021}
}
@article{ahuja2021invariance,
	title     = {Invariance principle meets information bottleneck for out-of-distribution generalization},
	author    = {Ahuja, Kartik and Caballero, Ethan and Zhang, Dinghuai and Gagnon-Audet, Jean-Christophe and Bengio, Yoshua and Mitliagkas, Ioannis and Rish, Irina},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Virtual},
	pages     = {3438--3450},
	year      = {2021}
}
@inproceedings{dubois2020learning,
	title     = {Learning optimal representations with the decodable information bottleneck},
	author    = {Dubois, Yann and Kiela, Douwe and Schwab, David J and Vedantam, Ramakrishna},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Virtual},
	pages     = {18674--18690},
	year      = {2020}
}
@inproceedings{pan2020disentangled,
	title     = {Disentangled information bottleneck},
	author    = {Pan, Ziqi and Niu, Li and Zhang, Jianfu and Zhang, Liqing},
	pages     = {9285--9293},
	booktitle = AAAI,
	publisher = {The {AAAI} Press},
	address   = {Virtual},
	year      = {2021}
}
@inproceedings{jeon2021ib,
	title     = {IB-GAN: Disengangled representation learning with information bottleneck generative adversarial networks},
	author    = {Jeon, Insu and Lee, Wonkwang and Pyeon, Myeongjang and Kim, Gunhee},
	booktitle = AAAI,
	publisher = {The {AAAI} Press},
	address   = {Virtual},
	pages     = {7926--7934},
	year      = {2021}
}
@inproceedings{zhou2020unified,
	title     = {Unified vision-language pre-training for image captioning and vqa},
	author    = {Zhou, Luowei and Palangi, Hamid and Zhang, Lei and Hu, Houdong and Corso, Jason and Gao, Jianfeng},
	booktitle = AAAI,
	publisher = {The {AAAI} Press},
	address   = {New York, NY, USA},
	pages     = {13041--13049},
	year      = {2020}
}
@inproceedings{kim2021vilt,
	title     = {ViLT: Vision-and-language transformer without convolution or region supervision},
	author    = {Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
	booktitle = ICML,
	publisher = {ACM},
	address   = {Virtual},
	pages     = {5583--5594},
	year      = {2021}
}
@inproceedings{huang2021seeing,
	title     = {Seeing out of the box: End-to-end pre-training for vision-language representation learning},
	author    = {Huang, Zhicheng and Zeng, Zhaoyang and Huang, Yupan and Liu, Bei and Fu, Dongmei and Fu, Jianlong},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Virtual},
	year      = {2021},
	pages     = {12976-12985}
}
@inproceedings{su2019vl,
	title     = {VL-BERT: Pre-training of generic visual-linguistic representations},
	author    = {Su, Weijie and Zhu, Xizhou and Cao, Yue and Li, Bin and Lu, Lewei and Wei, Furu and Dai, Jifeng},
	booktitle = ICLR,
	publisher = {Openreview},
	address   = {Addis Ababa, Ethiopia},
	year      = {2020}
}
@inproceedings{chen2020uniter,
	title     = {Uniter: Universal image-text representation learning},
	author    = {Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
	booktitle = ECCV,
	publisher = {Springer},
	address   = {Glasgow, UK},
	pages     = {104--120},
	year      = {2020}
}
@inproceedings{gan2020large,
	title     = {Large-scale adversarial training for vision-and-language representation learning},
	author    = {Gan, Zhe and Chen, Yen-Chun and Li, Linjie and Zhu, Chen and Cheng, Yu and Liu, Jingjing},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Virtual},
	pages     = {6616--6628},
	year      = {2020}
}
@inproceedings{li2020oscar,
	title     = {Oscar: Object-semantics aligned pre-training for vision-language tasks},
	author    = {Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others},
	booktitle = ECCV,
	publisher = {Springer},
	address   = {Glasgow, UK},
	pages     = {121--137},
	year      = {2020}
}
@inproceedings{zhang2021vinvl,
	title     = {Vinvl: Revisiting visual representations in vision-language models},
	author    = {Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng},
	booktitle = CVPR,
	publisher = {{IEEE}},
	address   = {Virtual},
	pages     = {5579--5588},
	year      = {2021}
}
@inproceedings{lu2019vilbert,
	title     = {{ViLBERT:} Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
	author    = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Vancouver, BC, Canada},
	pages     = {13--23},
	year      = {2019}
}
@inproceedings{lu202012,
	title     = {12-in-1: Multi-task vision and language representation learning},
	author    = {Lu, Jiasen and Goswami, Vedanuj and Rohrbach, Marcus and Parikh, Devi and Lee, Stefan},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Seattle, WA, USA},
	pages     = {10437--10446},
	year      = {2020}
}
@inproceedings{yu2020ernie,
	title     = {Ernie-vil: Knowledge enhanced vision-language representations through scene graphs},
	author    = {Yu, Fei and Tang, Jiji and Yin, Weichong and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},
	booktitle = AAAI,
	publisher = {The {AAAI} Press},
	address   = {Virtual},
	pages     = {3208--3216},
	year      = {2021}
}
@inproceedings{li2021scheduled,
	title     = {Scheduled sampling in vision-language pretraining with decoupled encoder-decoder network},
	author    = {Li, Yehao and Pan, Yingwei and Yao, Ting and Chen, Jingwen and Mei, Tao},
	booktitle = AAAI,
	publisher = {The {AAAI} Press},
	address   = {Virtual},
	pages     = {8518--8526},
	year      = {2021}
}
@inproceedings{agakov2004algorithm,
	title     = {The im algorithm: A variational approach to information maximization},
	author    = {Barber, David and Agakov, Felix},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Vancouver, BC, Canada},
	pages     = {201--208},
	year      = {2003}
}
@article{bennasar2015feature,
	title     = {Feature selection using joint mutual information maximisation},
	author    = {Bennasar, Mohamed and Hicks, Yulia and Setchi, Rossitza},
	journal   = {Expert Systems with Applications},
	volume    = {42},
	number    = {22},
	pages     = {8520--8532},
	year      = {2015},
	publisher = {Elsevier}
}
@article{li2019visualbert,
	title   = {Visualbert: A simple and performant baseline for vision and language},
	author  = {Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
	journal = {arXiv preprint arXiv:1908.03557},
	year    = {2019}
}
@inproceedings{cheng2020club,
	title     = {{CLUB}: A contrastive log-ratio upper bound of mutual information},
	author    = {Cheng, Pengyu and Hao, Weituo and Dai, Shuyang and Liu, Jiachang and Gan, Zhe and Carin, Lawrence},
	booktitle = ICML,
	publisher = {ACM},
	address   = {Virtual},
	pages     = {1779--1788},
	year      = {2020}
}
@inproceedings{poole2019variational,
	title     = {On variational bounds of mutual information},
	author    = {Poole, Ben and Ozair, Sherjil and Van Den Oord, Aaron and Alemi, Alex and Tucker, George},
	booktitle = ICML,
	publisher = {ACM},
	address   = {Long Beach, CA, USA},
	pages     = {5171--5180},
	year      = {2019}
}
@inproceedings{dancette2021beyond,
	title     = {Beyond question-based biases: Assessing multimodal shortcut learning in visual question answering},
	author    = {Dancette, Corentin and Cadene, Remi and Teney, Damien and Cord, Matthieu},
	booktitle = ICCV,
	publisher = {IEEE},
	address   = {Montreal, QC, Canada},
	pages     = {1574--1583},
	year      = {2021}
}
@article{chen2015microsoft,
	title   = {Microsoft coco captions: Data collection and evaluation server},
	author  = {Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
	journal = {arXiv preprint arXiv:1504.00325},
	year    = {2015}
}
@article{jiang2018pythia,
	title   = {Pythia v0. 1: the winning entry to the vqa challenge 2018},
	author  = {Jiang, Yu and Natarajan, Vivek and Chen, Xinlei and Rohrbach, Marcus and Batra, Dhruv and Parikh, Devi},
	journal = {arXiv preprint arXiv:1807.09956},
	year    = {2018}
}
@inproceedings{hu2018explainable,
	title     = {Explainable neural computation via stack neural module networks},
	author    = {Hu, Ronghang and Andreas, Jacob and Darrell, Trevor and Saenko, Kate},
	booktitle = ECCV,
	publisher = {Springer},
	address   = {Munich, Germany},
	pages     = {53--69},
	year      = {2018}
}
@inproceedings{shi2019xnm,
	title     = {Explainable and explicit visual reasoning over scene graphs},
	author    = {Shi, Jiaxin and Zhang, Hanwang and Li, Juanzi},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Long Beach, CA, USA},
	pages     = {8376--8384},
	year      = {2019}
}
@article{kazemi2017show,
	title   = {Show, ask, attend, and answer: A strong baseline for visual question answering},
	author  = {Kazemi, Vahid and Elqursh, Ali},
	journal = {arXiv preprint arXiv:1704.03162},
	year    = {2017}
}
@inproceedings{yang2016stacked,
	title     = {Stacked attention networks for image question answering},
	author    = {Yang, Zichao and He, Xiaodong and Gao, Jianfeng and Deng, Li and Smola, Alex},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Las Vegas, NV, USA},
	pages     = {21--29},
	year      = {2016}
}
@inproceedings{ben2019block,
	title     = {Block: Bilinear superdiagonal fusion for visual question answering and visual relationship detection},
	author    = {Ben-Younes, Hedi and Cadene, Remi and Thome, Nicolas and Cord, Matthieu},
	booktitle = AAAI,
	publisher = {The {AAAI} Press},
	address   = {Honolulu, HI, USA},
	pages     = {8102--8109},
	year      = {2019}
}
@inproceedings{shrestha2020negative,
	title     = {A negative case analysis of visual grounding methods for VQA},
	author    = {Shrestha, Robik and Kafle, Kushal and Kanan, Christopher},
	booktitle = ACL,
	publisher = {ACL},
	address   = {Virtual},
	pages     = {8172--8181},
	year      = {2020}
}
@inproceedings{nam2020learning,
	title     = {Learning from failure: De-biasing classifier from biased classifier},
	author    = {Nam, Junhyun and Cha, Hyuntak and Ahn, Sung-Soo and Lee, Jaeho and Shin, Jinwoo},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Virtual},
	pages     = {20673-20684},
	year      = {2020}
}
@inproceedings{nguyen2021revisiting,
	title     = {MoVie: Revisiting modulated convolutions for visual counting and beyond},
	author    = {Nguyen, Duy-Kien and Goswami, Vedanuj and Chen, Xinlei},
	booktitle = ICLR,
	publisher = {Openreview},
	address   = {Virtual},
	year      = {2021}
}
@article{kiela2019supervised,
	title   = {Supervised multimodal bitransformers for classifying images and text},
	author  = {Kiela, Douwe and Bhooshan, Suvrat and Firooz, Hamed and Perez, Ethan and Testuggine, Davide},
	journal = {arXiv preprint arXiv:1909.02950},
	year    = {2019}
}
@inproceedings{hu2021unit,
	title     = {Unit: Multimodal multitask learning with a unified transformer},
	author    = {Hu, Ronghang and Singh, Amanpreet},
	booktitle = ICCV,
	publisher = {IEEE},
	address   = {Montreal, QC, Canada},
	pages     = {1439--1449},
	year      = {2021}
}
@inproceedings{hu2020iterative,
	title     = {Iterative answer prediction with pointer-augmented multimodal transformers for textvqa},
	author    = {Hu, Ronghang and Singh, Amanpreet and Darrell, Trevor and Rohrbach, Marcus},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Seattle, WA, USA},
	pages     = {9992--10002},
	year      = {2020}
}
@article{oord2018representation,
	title   = {Representation learning with contrastive predictive coding},
	author  = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	journal = {arXiv preprint arXiv:1807.03748},
	year    = {2018}
}
@article{nguyen2010estimating,
	title     = {Estimating divergence functionals and the likelihood ratio by convex risk minimization},
	author    = {Nguyen, XuanLong and Wainwright, Martin J and Jordan, Michael I},
	journal   = {IEEE Transactions on Information Theory},
	volume    = {56},
	number    = {11},
	pages     = {5847--5861},
	year      = {2010},
	publisher = {IEEE}
}
@inproceedings{belghazi2018mine,
	title     = {mutual information neural estimation},
	author    = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeswar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, R Devon},
	booktitle = ICML,
	publisher = {ACM},
	address   = {Stockholmsm{\"{a}}ssan, Stockholm, Sweden},
	pages     = {530--539},
	year      = {2018}
}

% MiXPHM
@inproceedings{jia2021scaling,
	title     = {Scaling up visual and vision-language representation learning with noisy text supervision},
	author    = {Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
	booktitle = ICML,
	publisher = {ACM},
	address   = {Virtual},
	pages     = {4904--4916},
	year      = {2021}
}
@inproceedings{li2021align,
	title     = {Align before fuse: Vision and language representation learning with momentum distillation},
	author    = {Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Virtual},
	pages     = {9694--9705},
	year      = {2021}
}
@inproceedings{dou2022empirical,
	title     = {An empirical study of training end-to-end vision-and-language transformers},
	author    = {Dou, Zi-Yi and Xu, Yichong and Gan, Zhe and Wang, Jianfeng and Wang, Shuohang and Wang, Lijuan and Zhu, Chenguang and Zhang, Pengchuan and Yuan, Lu and Peng, Nanyun and others},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {New Orleans, LA, USA},
	pages     = {18166--18176},
	year      = {2022}
}
@inproceedings{zhong2022regionclip,
	title     = {Regionclip: Region-based language-image pretraining},
	author    = {Zhong, Yiwu and Yang, Jianwei and Zhang, Pengchuan and Li, Chunyuan and Codella, Noel and Li, Liunian Harold and Zhou, Luowei and Dai, Xiyang and Yuan, Lu and Li, Yin and others},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {New Orleans, LA, USA},
	pages     = {16793--16803},
	year      = {2022}
}
@inproceedings{wang2022ofa,
	title     = {OFA: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework},
	author    = {Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
	booktitle = ICML,
	publisher = {ACM},
	address   = {Baltimore, Maryland, USA},
	pages     = {23318--23340},
	year      = {2022}
}
@inproceedings{mahabadi2021parameter,
	title     = {Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks},
	author    = {Mahabadi, Rabeeh Karimi and Ruder, Sebastian and Dehghani, Mostafa and Henderson, James},
	booktitle = ACL,
	publisher = {ACL},
	address   = {Virtual},
	pages     = {565--576},
	year      = {2021}
}
@article{yang2022prompt,
	title   = {Prompt tuning for generative multimodal pretrained models},
	author  = {Yang, Hao and Lin, Junyang and Yang, An and Wang, Peng and Zhou, Chang and Yang, Hongxia},
	journal = {arXiv preprint arXiv:2208.02532},
	year    = {2022}
}
@inproceedings{ruckle2021adapterdrop,
	title     = {AdapterDrop: On the efficiency of adapters in transformers},
	author    = {R{\"u}ckl{\'e}, Andreas and Geigle, Gregor and Glockner, Max and Beck, Tilman and Pfeiffer, Jonas and Reimers, Nils and Gurevych, Iryna},
	booktitle = EMNLP,
	publisher = {ACL},
	address   = {Virtual},
	pages     = {7930--7946},
	year      = {2021}
}
@inproceedings{mao2022unipelt,
	title     = {UniPELT: A unified framework for parameter-efficient language model tuning},
	author    = {Mao, Yuning and Mathias, Lambert and Hou, Rui and Almahairi, Amjad and Ma, Hao and Han, Jiawei and Yih, Scott and Khabsa, Madian},
	booktitle = ACL,
	publisher = {ACL},
	address   = {Dublin, Ireland},
	pages     = {6253--6264},
	year      = {2022}
}
@inproceedings{he2022towards,
	title     = {Towards a unified view of parameter-efficient transfer learning},
	author    = {He, Junxian and Zhou, Chunting and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham},
	booktitle = ICLR,
	publisher = {OpenReview},
	address   = {Virtual},
	year      = {2022}
}
@inproceedings{yang2022robust,
	title     = {On robust prefix-tuning for text classification},
	author    = {Yang, Zonghan and Liu, Yang},
	booktitle = ICLR,
	publisher = {OpenReview},
	address   = {Virtual},
	year      = {2022}
}
@inproceedings{zhang2022differentiable,
	title     = {Differentiable prompt makes pre-trained language models better few-shot learners},
	author    = {Zhang, Ningyu and Li, Luoqiu and Chen, Xiang and Deng, Shumin and Bi, Zhen and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
	booktitle = ICLR,
	publisher = {OpenReview},
	address   = {Virtual},
	year      = {2022}
}
@inproceedings{zaken2022bitfit,
	title     = {Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models},
	author    = {Zaken, Elad Ben and Ravfogel, Shauli and Goldberg, Yoav},
	booktitle = ACL,
	publisher = {ACL},
	address   = {Dublin, Ireland},
	pages     = {1--9},
	year      = {2022}
}
@inproceedings{sung2021training,
	title     = {Training neural networks with fixed sparse masks},
	author    = {Sung, Yi-Lin and Nair, Varun and Raffel, Colin A},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Virtual},
	pages     = {24193--24205},
	year      = {2021}
}
@inproceedings{lester2021power,
	title     = {The power of scale for parameter-efficient prompt tuning},
	author    = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
	booktitle = EMNLP,
	publisher = {ACL},
	address   = {Virtual},
	pages     = {3045--3059},
	year      = {2021}
}
@inproceedings{li2021prefix,
	title     = {Prefix-tuning: Optimizing continuous prompts for generation},
	author    = {Li, Xiang Lisa and Liang, Percy},
	booktitle = ACL,
	publisher = {ACL},
	address   = {Virtual},
	pages     = {4582--4597},
	year      = {2021}
}
@inproceedings{houlsby2019parameter,
	title     = {Parameter-efficient transfer learning for NLP},
	author    = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
	booktitle = ICML,
	publisher = {ACM},
	address   = {Long Beach, California, USA},
	pages     = {2790--2799},
	year      = {2019}
}
@article{pfeiffer2020adapterfusion,
	title   = {AdapterFusion: Non-destructive task composition for transfer learning},
	author  = {Pfeiffer, Jonas and Kamath, Aishwarya and R{\"u}ckl{\'e}, Andreas and Cho, Kyunghyun and Gurevych, Iryna},
	journal = {arXiv preprint arXiv:2005.00247},
	year    = {2020}
}
@inproceedings{karimi2021compacter,
	title     = {Compacter: Efficient low-rank hypercomplex adapter layers},
	author    = {Karimi Mahabadi, Rabeeh and Henderson, James and Ruder, Sebastian},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Virtual},
	pages     = {1022--1035},
	year      = {2021}
}
@inproceedings{hu2022lora,
	title     = {Lora: Low-rank adaptation of large language models},
	author    = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	booktitle = ICLR,
	publisher = {OpenReview},
	address   = {Virtual},
	year      = {2022}
}
@inproceedings{lin2022adapt,
	title     = {ADAPT: Vision-language navigation with modality-aligned action prompts},
	author    = {Lin, Bingqian and Zhu, Yi and Chen, Zicong and Liang, Xiwen and Liu, Jianzhuang and Liang, Xiaodan},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {New Orleans, LA, USA},
	pages     = {15396-15406},
	year      = {2022}
}
@article{zhang2022hyperpelt,
	title   = {HyperPELT: Unified parameter-efficient language model tuning for both language and vision-and-language tasks},
	author  = {Zhang, Zhengkun and Guo, Wenya and Meng, Xiaojun and Wang, Yasheng and Wang, Yadao and Jiang, Xin and Liu, Qun and Yang, Zhenglu},
	journal = {arXiv preprint arXiv:2203.03878},
	year    = {2022}
}
@inproceedings{sung2022vl,
	title     = {Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks},
	author    = {Sung, Yi-Lin and Cho, Jaemin and Bansal, Mohit},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {New Orleans, LA, USA},
	pages     = {5227--5237},
	year      = {2022}
}
@inproceedings{tsimpoukelli2021multimodal,
	title     = {Multimodal few-shot learning with frozen language models},
	author    = {Tsimpoukelli, Maria and Menick, Jacob and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Virtual},
	pages     = {200--212},
	year      = {2021}
}
@inproceedings{yang2022empirical,
	title     = {An empirical study of gpt-3 for few-shot knowledge-based vqa},
	author    = {Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Lu, Yumao and Liu, Zicheng and Wang, Lijuan},
	booktitle = AAAI,
	publisher = {The AAAI Press},
	address   = {Virtual},
	pages     = {3081--3089},
	year      = {2022}
}
@article{oymak2019generalization,
	title   = {Generalization guarantees for neural networks via harnessing the low-rank structure of the jacobian},
	author  = {Oymak, Samet and Fabian, Zalan and Li, Mingchen and Soltanolkotabi, Mahdi},
	journal = {arXiv preprint arXiv:1906.05392},
	year    = {2019}
}
@inproceedings{aghajanyan2021intrinsic,
	title     = {Intrinsic dimensionality explains the effectiveness of language model fine-tuning},
	author    = {Aghajanyan, Armen and Gupta, Sonal and Zettlemoyer, Luke},
	booktitle = ACL,
	publisher = {ACL},
	address   = {Virtual},
	pages     = {7319--7328},
	year      = {2021}
}
@article{li2022parameter,
	title   = {Parameter-efficient sparsity for large language models fine-tuning},
	author  = {Li, Yuchao and Luo, Fuli and Tan, Chuanqi and Wang, Mengdi and Huang, Songfang and Li, Shen and Bai, Junjie},
	journal = {arXiv preprint arXiv:2205.11005},
	year    = {2022}
}
@inproceedings{shazeer2017outrageously,
	title     = {Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
	author    = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
	booktitle = ICLR,
	publisher = {OpenReview},
	address   = {Toulon, France},
	year      = {2017}
}
@inproceedings{lepikhin2021gshard,
	title     = {GShard: Scaling giant models with conditional computation and automatic sharding},
	author    = {Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
	booktitle = ICLR,
	publisher = {OpenReview},
	address   = {Virtual},
	year      = {2021}
}
@article{fedus2021switch,
	title   = {Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
	author  = {Fedus, William and Zoph, Barret and Shazeer, Noam},
	journal = {arXiv preprint arXiv:2101.03961},
	year    = {2021}
}
@article{lin2021m6,
	title   = {M6-10t: A sharing-delinking paradigm for efficient multi-trillion parameter pretraining},
	author  = {Lin, Junyang and Yang, An and Bai, Jinze and Zhou, Chang and Jiang, Le and Jia, Xianyan and Wang, Ang and Zhang, Jie and Li, Yong and Lin, Wei and others},
	journal = {arXiv preprint arXiv:2110.03888},
	year    = {2021}
}
@inproceedings{roller2021hash,
	title     = {Hash layers for large sparse models},
	author    = {Roller, Stephen and Sukhbaatar, Sainbayar and Weston, Jason and others},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Virtual},
	pages     = {17555--17566},
	year      = {2021}
}
@inproceedings{lewis2021base,
	title     = {Base layers: Simplifying training of large, sparse models},
	author    = {Lewis, Mike and Bhosale, Shruti and Dettmers, Tim and Goyal, Naman and Zettlemoyer, Luke},
	booktitle = ICML,
	publisher = {ACM},
	address   = {Virtual},
	pages     = {6265--6274},
	year      = {2021}
}
@inproceedings{riquelme2021scaling,
	title     = {Scaling vision with sparse mixture of experts},
	author    = {Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, Andr{\'e} and Keysers, Daniel and Houlsby, Neil},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Virtual},
	pages     = {8583--8595},
	year      = {2021}
}
@inproceedings{du2022glam,
	title     = {Glam: Efficient scaling of language models with mixture-of-experts},
	author    = {Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
	booktitle = ICML,
	publisher = {ACM},
	address   = {Baltimore, Maryland, USA},
	pages     = {5547--5569},
	year      = {2022}
}
@article{gao2022parameter,
	title   = {Parameter-efficient mixture-of-experts architecture for pre-trained language models},
	author  = {Gao, Ze-Feng and Liu, Peiyu and Zhao, Wayne Xin and Lu, Zhong-Yi and Wen, Ji-Rong},
	journal = {arXiv preprint arXiv:2203.01104},
	year    = {2022}
}
@article{wang2022adamix,
	title   = {AdaMix: Mixture-of-adapter for parameter-efficient tuning of large language models},
	author  = {Wang, Yaqing and Mukherjee, Subhabrata and Liu, Xiaodong and Gao, Jing and Awadallah, Ahmed Hassan and Gao, Jianfeng},
	journal = {arXiv preprint arXiv:2205.12410},
	year    = {2022}
}
@inproceedings{wang2022simvlm,
	title     = {SimVLM: Simple visual language model pretraining with weak supervision},
	author    = {Wang, Zirui and Yu, Jiahui and Yu, Adams Wei and Dai, Zihang and Tsvetkov, Yulia and Cao, Yuan},
	booktitle = ICLR,
	publisher = {OpenReview},
	address   = {Virtual},
	year      = {2022}
}
@inproceedings{zhang2021beyond,
	title     = {Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with $1/n$ parameters},
	author    = {Zhang, Aston and Tay, Yi and Zhang, Shuai and Chan, Alvin and Luu, Anh Tuan and Hui, Siu Cheung and Fu, Jie},
	booktitle = ICLR,
	publisher = {OpenReview},
	address   = {Virtual},
	year      = {2021}
}
@inproceedings{he2021effectiveness,
	title     = {On the effectiveness of adapter-based tuning for pretrained language model adaptation},
	author    = {He, Ruidan and Liu, Linlin and Ye, Hai and Tan, Qingyu and Ding, Bosheng and Cheng, Liying and Low, Jia-Wei and Bing, Lidong and Si, Luo},
	booktitle = ACL,
	publisher = {ACL},
	address   = {Virtual},
	pages     = {2208--2222},
	year      = {2021}
}
@article{laakso2000content,
	title     = {Content and cluster analysis: Assessing representational similarity in neural systems},
	author    = {Laakso, Aarre and Cottrell, Garrison},
	journal   = {Philosophical Psychology},
	volume    = {13},
	number    = {1},
	pages     = {47--76},
	year      = {2000},
	publisher = {Taylor \& Francis}
}
@inproceedings{zuo2022taming,
	title     = {Taming sparsely activated transformer with stochastic experts},
	author    = {Zuo, Simiao and Liu, Xiaodong and Jiao, Jian and Kim, Young Jin and Hassan, Hany and Zhang, Ruofei and Zhao, Tuo and Gao, Jianfeng},
	booktitle = ICLR,
	publisher = {OpenReview},
	address   = {Virtual},
	year      = {2022}
}
@article{jiang2022finetuning,
	title   = {Correlation Information Bottleneck: Towards adapting pretrained multimodal models for robust visual question answering},
	author  = {Jiang, Jingjing and Liu, Ziyi and Zheng, Nanning},
	journal = {arXiv preprint arXiv:2209.06954},
	year    = {2022}
}
@inproceedings{hjelm2019learning,
	title     = {Learning deep representations by mutual information estimation and maximization},
	author    = {Hjelm, R Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
	booktitle = ICLR,
	publisher = {OpenReview},
	address   = {New Orleans, LA, USA},
	year      = {2019}
}
@inproceedings{zbontar2021barlow,
	title     = {Barlow twins: Self-supervised learning via redundancy reduction},
	author    = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St{\'e}phane},
	booktitle = ICML,
	publisher = {ACM},
	address   = {Virtual},
	pages     = {12310--12320},
	year      = {2021}
}
@inproceedings{marino2019ok,
	title     = {Ok-vqa: A visual question answering benchmark requiring external knowledge},
	author    = {Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {Long Beach, CA, USA},
	pages     = {3195--3204},
	year      = {2019}
}
@article{chen2022revisiting,
	title   = {Revisiting parameter-efficient tuning: Are we really there yet?},
	author  = {Chen, Guanzheng and Liu, Fangyu and Meng, Zaiqiao and Liang, Shangsong},
	journal = {arXiv preprint arXiv:2202.07962},
	year    = {2022}
}
@inproceedings{perez2021true,
	title     = {True few-shot learning with language models},
	author    = {Perez, Ethan and Kiela, Douwe and Cho, Kyunghyun},
	booktitle = NIPS,
	publisher = {The MIT Press},
	address   = {Virtual},
	pages     = {11054--11070},
	year      = {2021}
}
@inproceedings{gao2021making,
	title     = {Making pre-trained language models better few-shot learners},
	author    = {Gao, Tianyu and Fisch, Adam and Chen, Danqi},
	booktitle = ACL,
	publisher = {ACL},
	address   = {Virtual},
	pages     = {3816--3830},
	year      = {2021}
}
@article{yuan2021florence,
	title   = {Florence: A new foundation model for computer vision},
	author  = {Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and others},
	journal = {arXiv preprint arXiv:2111.11432},
	year    = {2021}
}
@article{wang2022image,
	title   = {Image as a foreign language: BEiT pretraining for all vision and vision-language tasks},
	author  = {Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
	journal = {arXiv preprint arXiv:2208.10442},
	year    = {2022}
}
@inproceedings{yun2021pano,
	title     = {Pano-avqa: Grounded audio-visual question answering on 360deg videos},
	author    = {Yun, Heeseung and Yu, Youngjae and Yang, Wonsuk and Lee, Kangil and Kim, Gunhee},
	booktitle = ICCV,
	publisher = {IEEE},
	address   = {Montreal, QC, Canada},
	pages     = {2031--2041},
	year      = {2021}
}
@inproceedings{li2022learning,
	title     = {Learning to answer questions in dynamic audio-visual scenarios},
	author    = {Li, Guangyao and Wei, Yake and Tian, Yapeng and Xu, Chenliang and Wen, Ji-Rong and Hu, Di},
	booktitle = CVPR,
	publisher = {IEEE},
	address   = {New Orleans, LA, USA},
	pages     = {19108--19118},
	year      = {2022}
}
@inproceedings{wang2022rethinking,
	title     = {Rethinking minimal sufficient representation in contrastive learning},
	author    = {Wang, Haoqing and Guo, Xun and Deng, Zhi-Hong and Lu, Yan},
	booktitle = CVPR,
	pages     = {16041--16050},
	publisher = {IEEE},
	address   = {New Orleans, LA, USA},
	year      = {2022}
}
@inproceedings{wortsman2022model,
	title     = {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
	author    = {Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
	booktitle = ICML,
	pages     = {23965--23998},
	publisher = {ACM},
	address   = {Baltimore, Maryland, USA},
	year      = {2022}
}
@inproceedings{zeng2022multi,
	title     = {Multi-grained vision language pre-training: Aligning texts with visual concepts},
	author    = {Zeng, Yan and Zhang, Xinsong and Li, Hang},
	booktitle = ICML,
	pages     = {25994--26009},
	publisher = {ACM},
	address   = {Baltimore, Maryland, USA},
	year      = {2022}
}
@inproceedings{brown2020language,
	title     = {Language models are few-shot learners},
	author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	booktitle = NIPS,
	pages     = {1877--1901},
	publisher = {The MIT Press},
	address   = {Virtual},
	year      = {2020}
}
@misc{Lu2015,
	author       = {Jiasen Lu and Xiao Lin and Dhruv Batra and Devi Parikh},
	title        = {Deeper LSTM and normalized CNN Visual Question Answering model},
	year         = {2015},
	publisher    = {GitHub},
	journal      = {GitHub repository},
	howpublished = {\url{https://github.com/VT-vision-lab/VQA_LSTM_CNN}},
	commit       = {6c91cb9}
}
@inproceedings{yu2016modeling,
	title     = {Modeling context in referring expressions},
	author    = {Yu, Licheng and Poirson, Patrick and Yang, Shan and Berg, Alexander C and Berg, Tamara L},
	booktitle = ECCV,
	pages     = {69--85},
	publisher = {Springer},
	address   = {Netherlands},
	year      = {2016}
}
@inproceedings{liu2019adaptive,
	title     = {Adaptive reconstruction network for weakly supervised referring expression grounding},
	author    = {Liu, Xuejing and Li, Liang and Wang, Shuhui and Zha, Zheng-Jun and Meng, Dechao and Huang, Qingming},
	booktitle = ICCV,
	pages     = {2611--2620},
	publisher = {IEEE},
	address   = {Seoul, Korea},
	year      = {2019}
}
@inproceedings{zhang2020counterfactual,
	title     = {Counterfactual contrastive learning for weakly-supervised vision-language grounding},
	author    = {Zhang, Zhu and Zhao, Zhou and Lin, Zhijie and He, Xiuqiang and others},
	booktitle = NIPS,
	pages     = {18123--18134},
	publisher = {The MIT Press},
	address   = {Virtual},
	year      = {2020}
}
@inproceedings{frome2013devise,
	title   = {Devise: A deep visual-semantic embedding model},
	author  = {Frome, Andrea and Corrado, Greg S and Shlens, Jon and Bengio, Samy and Dean, Jeff and Ranzato, Marc'Aurelio and Mikolov, Tomas},
	booktitle = NIPS,
	pages     = {2121--2129},
	publisher = {The MIT Press},
	address   = {Lake Tahoe, NEV, US},
	year    = {2013}
}
@article{vinyals2016show,
	title     = {Show and tell: Lessons learned from the 2015 mscoco image captioning challenge},
	author    = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
	journal   = TPAMI,
	volume    = {39},
	number    = {4},
	pages     = {652--663},
	year      = {2016},
	publisher = {IEEE}
}
@inproceedings{anderson2018vision,
	title     = {Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments},
	author    = {Anderson, Peter and Wu, Qi and Teney, Damien and Bruce, Jake and Johnson, Mark and S{\"u}nderhauf, Niko and Reid, Ian and Gould, Stephen and Van Den Hengel, Anton},
	booktitle = CVPR,
	pages     = {3674--3683},
	publisher = {IEEE},
	address   = {Salt Lake City, UT, USA},
	year      = {2018}
}
