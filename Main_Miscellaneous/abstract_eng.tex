
% !TeX root = ../main.tex
\begin{englishabstract}
% \footnotetext{*The work was supported by the Foundation (foundation ID).}
% \astfootnote{The work was supported by the Foundation (foundation ID).}
% \blindtext

Visual Question Answering (VQA) is a significant task at the intersection of Computer Vision and Natural Language Processing. 
Its goal is to automatically reason a correct answer conditioned on a given image or video and a linguistic question related to visual content. 
Essentially, VQA is a multidisciplinary comprehensive problem that involves various techniques, including object recognition and detection, cross-modal retrieval, spatial and commonsense reasoning, and text generation. 
These techniques are unified into the single problem, making VQA akin to a visual Turing test. 
Additionally, VQA plays a crucial role in user-oriented human-computer interaction system and has broad practical application prospects, such as visually-impaired human assistance, medical consultation, safety monitoring, intelligent customer service, advertisement generation. 
Due to the significant theoretical research value and promising practical application prospects, VQA has recently become a research hotspot. 
In this dissertation, from the perspective of enhancing the practicality of VQA systems, we summarize four major challenges faced by VQA: (\emph{i}) insufficient utilization of multi-grained visual and linguistic information, (\emph{ii}) insufficient generalization ability caused by language biases, (\emph{iii}) insufficient robustness to variations in visual and linguistic input, and (\emph{iv}) insufficient learning ability in low-resource scenarios. 
We propose corresponding solutions to address these challenges, and the specific research contents and contributions are outlined below: 


\begin{enumerate}[wide,leftmargin=0pt]

\item To address the challenge of (\emph{i}) insufficient utilization of multi-grained visual and linguistic information, this dissertation proposes a lightweight visual-linguistic reasoning framework (LiVLR) for VideoQA. 
LiVLR leverages graph networks to separately encode visual and linguistic inputs at different semantic levels. 
Additionally, we design a diversity-aware visual-linguistic reasoning module (DaVL) to effectively integrate the obtained multi-grained visual and linguistic representations. 
DaVL adaptively learns the importance of different representations while maintaining representation discriminability, which markedly improves the effectiveness of cross-modal representation integration and the utilization of multi-grained visual and linguistic information. 
Experiments on three benchmarks show that the proposed LiVLR achieves the best performance with the least number of model parameters. 


\item To address the challenge of (\emph{ii}) insufficient generalization ability caused by language biases, this dissertation proposes a graph generative modeling-based training scheme (X-GGM) for out-of-distribution generalization in VQA. 
We first formulate the out-of-distribution generalization of VQA as a combinatorial generalization problem. 
Then, we utilize in-domain data to generate new combinations of existing visual concepts to prompt the VQA model to generate out-of-distribution answers. 
Moreover, we design a gradient distribution consistency loss to constrain the gradient distraction of data with adversarial perturbations and the generated data, efficiently alleviating the problem of unstable gradients in training. 
Experiments on two benchmarks show that X-GGM can significantly improve the out-of-distribution generalization ability of the baseline VQA model. 


\item To address the challenge of (\emph{iii}) insufficient robustness to variations in visual and linguistic input, this dissertation proposes correlation information bottleneck. 
This method aims to reduce task-independent redundancy in representations generated by pretrained visual-language models though encouraging the learned representations to converge to a minimum sufficient statistic and making the obtained representations more compact and more task-relevant. 
Experiments on five benchmarks show that the proposed method can significantly improve the input robustness of baseline VQA models. 


\item To address the challenge of (\emph{iv}) insufficient learning ability in low-resource scenarios, this dissertation proposes a redundancy-aware parameter-efficient tuning method (MixPHM) to adapt vision-language pretrained models to low-resource VQA. 
MixPHM transfers the knowledge implied in pretrained models to the downstream VQA in a parameter-efficient fashion and increases the capacity of models in a mixture-of-experts manner, which improves the stability and effectiveness of VQA models in low-resource learning scenarios.  
Experiments on multiple benchmarks show that the proposed method is the only parameter-efficient tuning method that outperforms full model finetuning on all datasets. 

\end{enumerate}
 

\englishkeywordstype{Visual question answering; Out-of-distribution generalization; Robustness; Low-resource learning; Graph neural network; Multimodal pretrained model}{Application Fundamentals}

% \englishkeywordstype{MHD equations; Finite element methods; Decoupled scheme; Stability; Convergence; Structure preserving; Preconditioning method}{Theoretical Research}

\end{englishabstract}
